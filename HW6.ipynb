{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\envs\\parallel\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded successfully...\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 64)           256000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 306,692\n",
      "Trainable params: 306,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13534 samples, validate on 1504 samples\n",
      "Epoch 1/20\n",
      " - 97s - loss: 0.5330 - acc: 0.7497 - val_loss: 0.4849 - val_acc: 0.7625\n",
      "Epoch 2/20\n",
      " - 97s - loss: 0.4560 - acc: 0.7747 - val_loss: 0.4637 - val_acc: 0.7794\n",
      "Epoch 3/20\n",
      " - 98s - loss: 0.4312 - acc: 0.7946 - val_loss: 0.4564 - val_acc: 0.7857\n",
      "Epoch 4/20\n",
      " - 102s - loss: 0.4153 - acc: 0.8055 - val_loss: 0.4615 - val_acc: 0.7817\n",
      "Epoch 5/20\n",
      " - 103s - loss: 0.4044 - acc: 0.8121 - val_loss: 0.4648 - val_acc: 0.7814\n",
      "Epoch 6/20\n",
      " - 96s - loss: 0.3966 - acc: 0.8173 - val_loss: 0.4723 - val_acc: 0.7744\n",
      "Epoch 7/20\n",
      " - 95s - loss: 0.3899 - acc: 0.8217 - val_loss: 0.4704 - val_acc: 0.7794\n",
      "Epoch 8/20\n",
      " - 89s - loss: 0.3822 - acc: 0.8248 - val_loss: 0.4763 - val_acc: 0.7812\n",
      "Epoch 9/20\n",
      " - 90s - loss: 0.3763 - acc: 0.8291 - val_loss: 0.4741 - val_acc: 0.7806\n",
      "Epoch 10/20\n",
      " - 93s - loss: 0.3690 - acc: 0.8319 - val_loss: 0.4988 - val_acc: 0.7778\n",
      "Epoch 11/20\n",
      " - 102s - loss: 0.3660 - acc: 0.8343 - val_loss: 0.4898 - val_acc: 0.7769\n",
      "Epoch 12/20\n",
      " - 102s - loss: 0.3596 - acc: 0.8375 - val_loss: 0.5100 - val_acc: 0.7786\n",
      "Epoch 13/20\n",
      " - 99s - loss: 0.3537 - acc: 0.8397 - val_loss: 0.5256 - val_acc: 0.7764\n",
      "Epoch 14/20\n",
      " - 100s - loss: 0.3492 - acc: 0.8422 - val_loss: 0.5333 - val_acc: 0.7764\n",
      "Epoch 15/20\n",
      " - 101s - loss: 0.3420 - acc: 0.8455 - val_loss: 0.5521 - val_acc: 0.7754\n",
      "Epoch 16/20\n",
      " - 103s - loss: 0.3367 - acc: 0.8481 - val_loss: 0.5520 - val_acc: 0.7756\n",
      "Epoch 17/20\n",
      " - 103s - loss: 0.3323 - acc: 0.8483 - val_loss: 0.5583 - val_acc: 0.7714\n",
      "Epoch 18/20\n",
      " - 106s - loss: 0.3250 - acc: 0.8535 - val_loss: 0.5709 - val_acc: 0.7754\n",
      "Epoch 19/20\n",
      " - 103s - loss: 0.3190 - acc: 0.8565 - val_loss: 0.6054 - val_acc: 0.7714\n",
      "Epoch 20/20\n",
      " - 105s - loss: 0.3124 - acc: 0.8592 - val_loss: 0.6069 - val_acc: 0.7706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#參數\n",
    "KEY = 'gender'\n",
    "TEXT = 'description'\n",
    "NUM_WORDS = 1200\n",
    "MAX_LENGTH = 500\n",
    "HIDDEN_SIZE = 256\n",
    "OUTPUT_DIM = 64\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#----------------------------------------------------------\n",
    "FILE_PATH = 'gender-classifier-DFE-791531.csv'\n",
    "raw_data = pd.read_csv(FILE_PATH, encoding = \"ISO-8859-1\")\t\t\t\t\t\t\n",
    "raw_data[KEY] = raw_data[KEY].fillna('unknown')\n",
    "raw_data[KEY] = raw_data[KEY].map({'male':0,'female':1,'brand':2,'unknown':3}).astype(int)\n",
    "raw_data[TEXT] = raw_data[TEXT].fillna('NaN')\n",
    "print(\"Raw data loaded successfully...\\n\")\n",
    "#資料處理\n",
    "data_0 = raw_data[raw_data[KEY] == 0] \n",
    "data_1 = raw_data[raw_data[KEY] == 1] \n",
    "data_2 = raw_data[raw_data[KEY] == 2] \n",
    "data_3 = raw_data[raw_data[KEY] == 3] \n",
    "\n",
    "raw_data = pd.concat([data_0,data_1], axis = 0, ignore_index=True)\n",
    "raw_data = pd.concat([raw_data,data_2], axis = 0, ignore_index=True)\n",
    "raw_data = pd.concat([raw_data,data_3], axis = 0, ignore_index=True)\n",
    "\n",
    "train_data = raw_data.sample(frac = 0.75, random_state = 1)\n",
    "test_data = raw_data.loc[~raw_data.index.isin(train_data.index)]\n",
    "\n",
    "train_x = train_data[TEXT]\n",
    "test_x = test_data[TEXT]\n",
    "\n",
    "train_y = to_categorical(train_data[KEY])\n",
    "test_y = to_categorical(test_data[KEY])\n",
    "\n",
    "token = Tokenizer(num_words = NUM_WORDS)\n",
    "token.fit_on_texts(train_x)\n",
    "\n",
    "train_x = sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen = MAX_LENGTH)\n",
    "test_x = sequence.pad_sequences(token.texts_to_sequences(test_x),maxlen = MAX_LENGTH)\n",
    "#模型建立\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim = OUTPUT_DIM, input_dim = 4000, input_length = MAX_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "#LSTM\n",
    "model.add(LSTM(OUTPUT_DIM))\n",
    "#HIDDEN_LAYERS\n",
    "model.add(Dense(units = HIDDEN_SIZE, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#OUTPUT_LAYERS\n",
    "model.add(Dense(units = 4, activation = 'sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "train_history = model.fit(x = train_x,y = train_y,validation_split = VALIDATION_SPLIT,epochs= EPOCHS, batch_size = BATCH_SIZE,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
